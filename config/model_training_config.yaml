# ==============================
# MODEL TRAINING CONFIGURATION
# ==============================
# Production-grade ML training pipeline with MLflow tracking

# ==============================
# PROJECT METADATA
# ==============================
project:
  name: 'Maxwell Selassie Hiamatsu'
  version: 1.0
  description: 'Loan Eligibility Prediction - Model Training'

# ==============================
# DATA PATHS
# ==============================
data:
  train_data: 'data/processed/train_set.csv'
  val_data: 'data/processed/dev_set.csv'
  test_data: 'data/processed/test_set.csv'
  target_column: 'stroke'
  
  # Preprocessing artifacts
  scaler_path: 'models/scaler.pkl'
  encoder_path: 'models/encoder.pkl'

# ==============================
# MLFLOW CONFIGURATION
# ==============================
mlflow:
  tracking_uri: './mlruns'
  experiment_name: 'Loan_Eligibility_Prediction'
  run_name_pattern: 'run_{model_name}_{timestamp}'
  
  # What to track
  track:
    hyperparameters: True
    metrics: True
    model_artifacts: True
    feature_importance: True
    plots: True
    training_duration: True
    dataset_metadata: True
    preprocessing_artifacts: True
  
  # Model Registry
  model_registry:
    enabled: True
    register_best_model: True
    model_name: 'LoanEligibilityClassifier'
    stage: 'Staging'  # None, Staging, Production, Archived
    
    # Require manual promotion
    auto_promote_to_production: False
    
    # Performance gates
    min_performance_threshold:
      f1_score: 0.70
      roc_auc: 0.75
    
    # Compare with production
    compare_with_production: True
    min_improvement_required: 0.01

model_validation:
  enabled: True
  
  thresholds:
    f1_score:
      min: 0.70
      warning: 0.75
      target: 0.85
    
    roc_auc:
      min: 0.75
      warning: 0.80
      target: 0.90
    
    train_test_gap:
      max: 0.15
      warning: 0.10
  
  business_rules:
    min_recall_for_positive_class: 0.80
    max_false_positive_rate: 0.30

# ==============================
# MODELS CONFIGURATION
# ==============================
models:
  # Baseline Model
  baseline:
    name: 'LogisticRegression'
    enabled: True
    params:
      random_state: 42
      max_iter: 1000
      class_weight: 'balanced'
      solver: 'lbfgs'
  
  # Tree-Based Models
  tree_based:
    RandomForest:
      enabled: True
      params:
        random_state: 42
        class_weight: 'balanced'
        n_estimators: 100
        n_jobs: -1
    
    XGBoost:
      enabled: True
      params:
        random_state: 42
        scale_pos_weight: 2.14  # ratio of N to Y (calculated from data)
        n_estimators: 100
        learning_rate: 0.1
        n_jobs: -1
        eval_metric: 'logloss'
    
    LightGBM:
      enabled: True
      params:
        random_state: 42
        class_weight: 'balanced'
        n_estimators: 100
        learning_rate: 0.1
        n_jobs: -1
        verbose: -1

# ==============================
# CROSS-VALIDATION
# ==============================
cross_validation:
  enabled: True
  method: 'stratified_kfold'
  n_splits: 10
  shuffle: True
  random_state: 42
  
  # Report CV scores
  report_cv_scores: True
  aggregate_method: 'mean'  # mean, median

# ==============================
# HYPERPARAMETER TUNING
# ==============================
hyperparameter_tuning:
  enabled: True
  method: 'optuna'
  tune_top_n_models: 3
  n_trials: 50
  cv_splits: 5  # Use 5-fold CV during tuning (faster)
  timeout: 3600  # 1 hour timeout per model
  
  # Optuna settings
  optuna:
    direction: 'maximize'  # maximize F1-score
    sampler: 'TPESampler'  # Tree-structured Parzen Estimator
    pruner: 'MedianPruner'  # Early stopping for bad trials
    n_jobs: 1  # Sequential for stability
  
  # Hyperparameter search spaces
  search_spaces:
    LogisticRegression:
      C: ['log_uniform', 0.001, 100]
      penalty: ['categorical', ['l1', 'l2']]
      solver: ['categorical', ['liblinear', 'saga']]
    
    RandomForest:
      n_estimators: ['int', 50, 500]
      max_depth: ['int', 3, 30]
      min_samples_split: ['int', 2, 20]
      min_samples_leaf: ['int', 1, 10]
      max_features: ['categorical', ['sqrt', 'log2']]
    
    XGBoost:
      n_estimators: ['int', 50, 500]
      max_depth: ['int', 3, 15]
      learning_rate: ['log_uniform', 0.001, 0.3]
      subsample: ['uniform', 0.6, 1.0]
      colsample_bytree: ['uniform', 0.6, 1.0]
      gamma: ['uniform', 0, 5]
      min_child_weight: ['int', 1, 10]
    
    LightGBM:
      n_estimators: ['int', 50, 500]
      max_depth: ['int', 3, 15]
      learning_rate: ['log_uniform', 0.001, 0.3]
      num_leaves: ['int', 20, 150]
      min_child_samples: ['int', 5, 100]
      subsample: ['uniform', 0.6, 1.0]
      colsample_bytree: ['uniform', 0.6, 1.0]

# ==============================
# EVALUATION METRICS
# ==============================
metrics:
  primary_metric: 'accuracy'
  
  classification_metrics:
    - 'accuracy'
    - 'precision'
    - 'recall'
    - 'f1_score'
    - 'roc_auc'
    - 'average_precision'  # PR-AUC
  
  # Calibration metrics
  calibration_metrics:
    enabled: True
    metrics:
      - 'brier_score'
      - 'log_loss'
    n_bins: 10
  
  # Confusion matrix
  confusion_matrix:
    enabled: True
    normalize: 'true'  # 'true', 'pred', 'all', None
  
  # Classification report
  classification_report:
    enabled: True
    output_dict: True

# ==============================
# FEATURE IMPORTANCE
# ==============================
feature_importance:
  enabled: True
  
  methods:
    native:
      enabled: True
      applicable_to: ['RandomForest', 'XGBoost', 'LightGBM']
    
    permutation:
      enabled: True
      n_repeats: 10
      random_state: 42
      applicable_to: 'all'
    
    shap:
      enabled: False  # Disabled for speed (enable for deep analysis)
      applicable_to: 'all'
  
  # Plotting
  plot:
    enabled: True
    top_n_features: 20
    save_format: 'png'
    dpi: 300

# ==============================
# MODEL SELECTION
# ==============================
model_selection:
  # Automatic selection
  auto_select:
    enabled: True
    criteria: 'primary_metric'  # primary_metric, balanced_score
  
  # Selection criteria for manual verification
  criteria:
    primary_metric:
      weight: 0.5
      threshold: 0.75  # Minimum F1-score
    
    training_time:
      weight: 0.15
      max_acceptable: 300  # 5 minutes
    
    interpretability:
      weight: 0.15
      preference: ['LogisticRegression', 'RandomForest', 'XGBoost', 'LightGBM']
    
    generalization:
      weight: 0.20
      measure: 'val_train_gap'  # Difference between val and train score
      max_acceptable_gap: 0.10  # 10% gap acceptable

# ==============================
# CLASS IMBALANCE
# ==============================
class_imbalance:
  method: 'class_weight_balanced'
  
  # Alternative methods (disabled by default)
  smote:
    enabled: False
    sampling_strategy: 'auto'
    random_state: 42
  
  undersampling:
    enabled: False
    sampling_strategy: 'auto'
    random_state: 42

# ==============================
# MODEL PERSISTENCE
# ==============================
model_persistence:
  save_best_model_only: True
  best_model_path: 'models/best_model.joblib'
  
  # Save additional artifacts
  save_artifacts:
    feature_names: True
    class_names: True
    metrics_summary: True
    training_history: True
  
  artifacts_path: 'models/artifacts/'

# ==============================
# REPRODUCIBILITY
# ==============================
reproducibility:
  random_state: 42
  set_seeds:
    numpy: True
    sklearn: True
    xgboost: True
    lightgbm: True
    optuna: True

# ==============================
# LOGGING
# ==============================
logging:
  log_dir: 'logs/'
  log_level: 'INFO'
  log_file: 'model_training.log'
  max_bytes: 10485760  # 10MB
  backup_count: 7
  
  track_during_training:
    - 'training_time'
    - 'memory_usage'
    - 'data_shapes'
    - 'class_distribution'
    - 'feature_count'
    - 'convergence_warnings'

# ==============================
# PLOTTING
# ==============================
plotting:
  enabled: True
  plots_dir: 'artifacts/plots/'
  
  plots_to_generate:
    - 'confusion_matrix'
    - 'roc_curve'
    - 'precision_recall_curve'
    - 'feature_importance'
    - 'calibration_curve'
    - 'learning_curve'
  
  plot_format: 'png'
  dpi: 300
  style: 'seaborn-v0_8-darkgrid'

# ==============================
# ERROR HANDLING
# ==============================
error_handling:
  on_model_failure: 'stop'  # stop, skip, retry
  max_retries: 0
  save_failed_models: False

# ==============================
# NOTES
# ==============================
notes:
  - 'Models trained on preprocessed and feature-engineered data'
  - 'Class imbalance handled with class_weight=balanced'
  - 'Hyperparameter tuning uses Optuna with TPE sampler'
  - 'Best model selected based on F1-score with manual verification'
  - 'All experiments tracked in MLflow'
  - 'Best model registered in MLflow Model Registry as Production'

assumptions:
  - 'Data is already preprocessed and split (train/test)'
  - 'Target variable is binary (0/1)'
  - 'All features are numeric after preprocessing'
  - 'Class imbalance ratio is approximately 2:1'